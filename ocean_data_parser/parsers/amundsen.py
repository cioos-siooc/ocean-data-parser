"""[Amundsen Science](https://amundsenscience.com).

This module contains the parser for the Amundsen INT format.
This format is used to store oceanographic data generated by
the [Amundsen Science](https://amundsenscience.com/) and
[ArcticNet](https://arcticnet.ulaval.ca/) programs.
"""

import logging
import re
from collections import Counter
from pathlib import Path

import pandas as pd
import xarray as xr
from gsw import z_from_p

from ocean_data_parser.parsers.utils import standardize_dataset
from ocean_data_parser.vocabularies.load import amundsen_vocabulary

logger = logging.getLogger(__name__)
string_attributes = ["Cruise_Number", "Cruise_Name", "Station"]

default_global_attributes = {"unknown_variables_information": "", "history": ""}
VARIABLE_MAPPING_FIXES = {
    "% Fluorescence [ug/L]": "Fluo",
    "% Conservative Temperature (TEOS-10) [deg C]": "CONT",
    "% In situ density TEOS10 ((s, t, p) - 1000) [kg/m^3]": "D_CT",
    "% Potential density TEOS10 ((s, t, 0) - 1000) [kg/m^3]": "D0CT",
    "% Potential density TEOS10 (s, t, 0) [kg/m^3]": "D0CT",
}

IGNORED_VARIABLES = [
    "Date",
    "Hour",
    "svan",
    "Svan",
    "SV",
    "SVEL",
    "FreezT",
    "FreezT-",
    "CONT",
    "Cont",
    "D_CT",
    "D_ct",
    "D0CT",
    "D0ct",
    "SIGM",
    "SPVA",
    "VAIS",
    "POTM",
    "FRET",
    "SIGP",
    "FLUOV",
]

# Fix variables with white space in the name
REPLACE_VARIABLES = {
    "Wind dir": "Wind_dir",
    "Wind speed": "Wind_speed",
    "Air temp": "Air_temp",
    "Dew point": "Dew_point",
}


def _standardize_attribute_name(name: str) -> str:
    """Standardize attribute names.

    The function applies the following transformations:
        - All lower case
        - All symbols " []" are replaced by underescores
        - Trailing underscores removed.

    Args:
        name (string): original attribute name

    Returns (string): converted attribute name
    """
    formatted_name = re.sub(r"[\s\[\]]+", "_", name.lower())
    formatted_name = re.sub("_$", "", formatted_name)
    return formatted_name


def _standardize_attribute_value(value: str, name: str = None):
    """Cast attribute value to the appropriate format.

    Args:
        value (string): [description]
        name (string, optional): [description]. Defaults to None.

    Returns:
        [str,float,int,pd.Timestamp]: cast attribute value according to the right format.
    """
    if name in string_attributes or not isinstance(value, str):
        return value
    elif re.fullmatch(r"\d\d-\w\w\w-\d\d\d\d \d\d\:\d\d\:\d\d", value):
        return pd.to_datetime(
            value, utc=(name and "utc" in name), format="%d-%b-%Y %H:%M:%S"
        )
    elif re.fullmatch(r"\d\d-\w\w\w-\d\d\d\d \d\d\:\d\d\:\d\d.\d+", value):
        return pd.to_datetime(
            value, utc=(name and "utc" in name), format="%d-%b-%Y %H:%M:%S.%f"
        )
    elif re.match(r"^-{0,1}\d+\.\d+$", value):
        return float(value)
    elif re.match(r"^-{0,1}\d+$", value):
        return int(value)
    else:
        return value


def _extract_variable_attributes_from_header(header: str, variables: list) -> dict:
    """Extract variable attributes from the header."""
    return {
        column: re.search(
            r"\s*(?P<long_name>[^\[]+)\s*(\[(?P<units>.*)\]){0,1}",
            header.pop(column[0].upper() + column[1:]),
        ).groupdict()
        if column[0].upper() + column[1:] in header
        else {}
        for column in variables
    }


def _convert_timestamp(df: pd.DataFrame) -> pd.DataFrame:
    """Convert Date and Hour columns to a single time column.

    Fix the issue with the 60 seconds in the Hour column
    which is not supported by pandas.
    """
    is_60 = df["Hour"].str.contains(":60$")
    df.loc[is_60, "Hour"] = df.loc[is_60, "Hour"].str.replace(":60$", ":00", regex=True)
    df["time"] = pd.to_datetime(df["Date"] + "T" + df["Hour"], utc=True)
    df.loc[is_60, "time"] += pd.Timedelta(seconds=60)
    return df


def _get_file_type(path: str) -> str:
    """Get the file type from the file path."""
    file_type = re.search("AVOS|TSG|Bioness|NAV|Hydrobios", Path(path).name)
    return file_type.group() if file_type else None


def csv_format(
    path: str,
    encoding: str = "UTF-8",
    map_to_vocabulary: bool = True,
    generate_depth: bool = True,
    separator: str = ",",
    encoding_error="strict",
) -> xr.Dataset:
    """Parse Amundsen CSV format.

    Args:
        path (str): file path to parse.
        encoding (str, optional): File encoding. Defaults to "Windows-1252".
        map_to_vocabulary (bool, optional): Rename variables to vocabulary. Defaults to True.
        generate_depth (bool, optional): Generate depth variable. Defaults to True.
        separator (str, optional): Separator for the data. Defaults to r",".
        encoding_error (str, optional): Encoding error handling. Defaults to "strict".

    Returns:
        xr.Dataset
    """
    return int_format(
        path=path,
        encoding=encoding,
        map_to_vocabulary=map_to_vocabulary,
        generate_depth=generate_depth,
        separator=separator,
        encoding_error=encoding_error,
    )


def int_format(
    path: str,
    encoding: str = "Windows-1252",
    map_to_vocabulary: bool = True,
    generate_depth: bool = True,
    separator: str = r"\s+",
    encoding_error="strict",
) -> xr.Dataset:
    r"""Parse Amundsen INT format.

    Args:
        path (str): file path to parse.
        encoding (str, optional): File encoding. Defaults to "Windows-1252".
        map_to_vocabulary (bool, optional): Rename variables to vocabulary. Defaults to True.
        generate_depth (bool, optional): Generate depth variable. Defaults to True.
        separator (str, optional): Separator for the data. Defaults to r"\s+".
        encoding_error (str, optional): Encoding error handling. Defaults to "strict".

    Returns:
        xr.Dataset
    """
    metadata = default_global_attributes.copy()

    # Ignore info.int files
    if path.endswith("_info.int"):
        logger.warning("Ignore *_info.int files: %s", path)
        return

    logger.debug("Read %s", path)
    with open(path, encoding=encoding, errors=encoding_error) as file:
        # Parse header
        for header_line_idx, line in enumerate(file):
            line = line.rstrip()
            if re.match(r"^%\s*$", line) or not line:
                continue
            elif (
                line and not re.match(r"\s*%", line) and (line[0] == " " or "," in line)
            ):
                break
            elif ":" in line:
                key, value = line.strip()[1:].split(":", 1)
                metadata[key.strip()] = value.strip()
            elif line in VARIABLE_MAPPING_FIXES:
                metadata[VARIABLE_MAPPING_FIXES[line]] = line[2:]
            elif re.match(r"% .* \[.+\]", line):
                logger.warning(
                    "Unknown variable name will be saved to unknown_variables_information: %s",
                    line,
                )
                metadata["unknown_variables_information"] += line + "\n"
            else:
                logger.warning("Unknown line format: %s", line)

        # Review metadata
        if metadata == default_global_attributes:
            logger.warning("No metadata was captured in the header of the INT file.")

    # Parse data
    if separator == r"\s+":
        # Fix problematic variable names
        for name, fixed_name in REPLACE_VARIABLES.items():
            line = line.replace(name, fixed_name)
        names = re.split(r"\s+", line.strip())
    elif separator == ",":
        names = line.strip().split(",")

    if len(set(names)) != len(names):
        duplicated = {name: n for name, n in Counter(names).items() if n > 1}
        logger.warning("Duplicated variable names detected: %s", duplicated)
        # Add index to duplicate names
        new_names = []
        for name in names:
            if name not in new_names:
                new_names.append(name)
            else:
                new_names.append(f"{name}_{new_names.count(name)}")
        names = new_names

    df = pd.read_csv(
        path,
        encoding=encoding,
        header=header_line_idx,
        skiprows=[header_line_idx] if separator == r"\s+" else [],
        sep=separator,
        names=names,
        encoding_errors=encoding_error,
    )
    if len(df.columns) != len(names):
        raise ValueError(
            f"Number of columns ({len(df.columns)}) doesn't match the number of variables ({len(names)})"
        )

    # Sort column attributes
    variables = _extract_variable_attributes_from_header(metadata, df.columns)
    if "Date" in df and "Hour" in df:
        df = _convert_timestamp(df)

    # Convert to xarray object
    ds = df.to_xarray()

    # Standardize global attributes
    ds.attrs = {
        _standardize_attribute_name(name): _standardize_attribute_value(
            value, name=name
        )
        for name, value in metadata.items()
    }

    # Fix global attribute
    ds = _fix_station_global_attribute(ds)

    # Generate instrument_depth variable
    pressure = [var for var in ds if var in ("Pres", "PRES")]
    if (
        generate_depth
        and pressure
        and ("Lat" in ds or "initial_latitude_deg" in ds.attrs)
    ):
        logger.info(
            "Generate instrument_depth from TEOS-10: -1 * gsw.z_from_p(ds['Pres'], %s)",
            "ds['Lat']" if "Lat" in ds else "ds.attrs['initial_latitude_deg']",
        )
        ds["instrument_depth"] = -z_from_p(
            ds[pressure[0]],
            ds["Lat"] if "Lat" in ds else ds.attrs["initial_latitude_deg"],
        )

    # Map variables to vocabulary
    file_type = _get_file_type(path)
    variables_vocabulary = amundsen_vocabulary(file_type)
    variables_to_rename = {}
    for var in ds:
        if var not in variables or re.sub(r"_\d+$", "", var) in IGNORED_VARIABLES:
            continue

        ds[var].attrs = variables[var]
        if "long_name" in ds[var].attrs:
            ds[var].attrs["long_name"] = ds[var].attrs["long_name"].strip()

        # Include variable attributes from the vocabulary
        if not map_to_vocabulary:
            continue
        elif var not in variables_vocabulary:
            logger.warning(
                "No vocabulary is available for variable mapping '%s': %s",
                var,
                ds[var].attrs,
            )
            continue

        # Match vocabulary
        var_units = ds[var].attrs.get("units")
        for item in variables_vocabulary[var]:
            accepted_units = item.get("accepted_units")
            if (
                var_units is None  # Consider first if no units
                or var_units == item.get("units")
                or (accepted_units and re.fullmatch(accepted_units, var_units))
                or ("None" in accepted_units)
            ):
                if "rename" in item:
                    variables_to_rename[var] = item["rename"]

                ds[var].attrs = {
                    key: value
                    for key, value in item.items()
                    if key not in ["accepted_units", "rename", "file_type"]
                }
                break
        else:
            logger.warning(
                "No Vocabulary available for %s [%s]: %s",
                var,
                var_units,
                str(ds[var].attrs),
            )

    # Review rename variables
    already_existing_variables = {
        var: rename for var, rename in variables_to_rename.items() if rename in ds
    }
    if already_existing_variables:
        logger.error(
            "Can't rename variable %s since it already exist",
            already_existing_variables,
        )

    if variables_to_rename:
        logger.info("Rename variables: %s", variables_to_rename)
        ds = ds.rename(variables_to_rename)

    # Assign dimensions only after renaming variables
    ds = _assign_dimensions(ds, instrument=file_type)

    # Standardize dataset to be compatible with ERDDAP and NetCDF Classic
    ds = standardize_dataset(ds)
    return ds


COORDINATES_VARIABLES = ["time", "latitude", "longitude", "PRES"]


def _assign_dimensions(ds: xr.Dataset, instrument: str) -> xr.Dataset:
    """Assign dimensions to the dataset."""
    # Assign dimensions
    ds = ds.set_coords(
        [coordinate for coordinate in COORDINATES_VARIABLES if coordinate in ds]
    )
    if "latitude" in ds and "longitude" in ds and "time" in ds:
        ds.attrs.update(
            {
                "cdm_data_type": "Trajectory",
            }
        )
        ds = ds.swap_dims({"index": "time"})
        ds = ds.drop_vars("index")
    elif "time" in ds:
        ds.attrs.update(
            {
                "cdm_data_type": "TimeSeries",
            }
        )
        ds = ds.swap_dims({"index": "time"})
        ds = ds.drop_vars("index")
    elif "PRES" in ds:
        ds.attrs.update(
            {
                "cdm_data_type": "Profile",
            }
        )
        ds = ds.swap_dims({"index": "PRES"})
        ds = ds.drop_vars("index")
    elif instrument in ("Bioness", "Hydrobios"):
        ds.attrs.update(
            {
                "cdm_data_type": "Profile",
            }
        )
        ds = ds.swap_dims({"index": "Net"})
        ds = ds.drop_vars("index")
    else:
        logger.warning("Unknown CDM data type")
    return ds


def _fix_station_global_attribute(ds: xr.Dataset):
    """Fix station global attribute."""
    if "station" not in ds.attrs:
        return ds
    station = ds.attrs["station"]
    station = re.sub(r"station\s*", "", station, flags=re.IGNORECASE).strip()
    ds.attrs["station"] = station
    return ds
