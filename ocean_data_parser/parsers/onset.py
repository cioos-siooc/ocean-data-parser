"""[Onset](https://www.onsetcomp.com/) is a company that manufactures
data loggers and sensors for environmental monitoring.
Their Hobo data loggers are widely used for monitoring water
quality parameters such as temperature, conductivity, and light
intensity. The present module provides parsers for the different
data formats generated by the HOBOware and HOBOconnect softwares.
"""

import logging
import re
from csv import reader
from datetime import datetime

import pandas as pd
import xarray

from ocean_data_parser.parsers.checks import check_daylight_saving
from ocean_data_parser.parsers.utils import standardize_dataset

GLOBAL_ATTRIBUTES = {"instrument_manufacturer": "Onset", "Convention": "CF-1.6"}

TIMEZONE_MAPPING = {
    "GMT": "UTC",
    "PDT": "US/Pacific",
    "PST": "US/Pacific",
}

logger = logging.getLogger(__name__)
VARIABLE_NAME_MAPPING = {
    "#": "record_number",
    "\ufeff#": "record_number",
    "Date Time": "time",
    "Temp": "temperature",
    "Intensity": "light_intensity",
    "Specific Conductance": "specific_conductance",
    "Low Range": "low_range",
    "EOF": "end_of_file",
    "End of File": "end_of_file",
    "Abs Pres Barom.": "barometric_pressure",
    "Pressure Barom.": "barometric_pressure",
    "Abs Pres": "pressure",
    "Sensor Depth": "sensor_depth",
    "Turbidity": "turbidity",
    "Water Level": "water_level",
}

IGNORED_VARIABLES = [
    "record_number",
    "time",
    "button_up",
    "button_down",
    "host_connected",
    "end_of_file",
    "coupler_detached",
    "coupler_attached",
    "stopped",
    "started",
    "good_battery",
    "bad_battery",
    "host_connect",
    "batt",
    "low_power",
    "water_detect",
    "record",
    "eof",
    "",
]

DATETIME_REGEX_FORMATS = [
    (r"\d\d\/\d\d\/\d\d\s+\d\d\:\d\d\:\d\d\s+\w\w", r"%m/%d/%y %I:%M:%S %p"),
    (r"\d+\/\d+\/\d\d\d\d\s+\d+\:\d+\:\d+", r"%m/%d/%Y %H:%M:%S"),
    (r"\d\d\d\d\/\d\d\/\d\d\s+\d\d\:\d\d\:\d\d\s+\w\w", r"%Y/%m/%d %I:%M:%S %p"),
    (r"\d\d\/\d\d\/\d\d\s+\d\d\:\d\d", r"%m/%d/%y %H:%M"),
    (r"\d\d\/\d\d\/\d\d\s+\d\d\:\d\d:\d\d", r"%y/%m/%d %H:%M:%S"),
    (r"\d\d\/\d\d\/\d\d\d\d\s+\d\d\:\d\d\:\d\d\s+(AM|PM)", r"%m/%d/%Y %H:%M:%S %p"),
    (r"\d+\/\d+\/\d\d\s+\d\d\:\d\d", r"%m/%d/%y %H:%M"),
    (r"^\d\d\d\d\-\d\d\-\d\d\s+\d\d\:\d\d\:\d\d$", r"%Y-%m-%d %H:%M:%S"),
    (r"\d\d\d\d\-\d\d\-\d\d\s+\d\d\:\d\d\:\d\d (AM|PM)", r"%Y-%m-%d %I:%M:%S %p"),
    (r"^\d\d\-\d\d\-\d\d\s+\d{1,2}\:\d\d$", r"%y-%m-%d %H:%M"),
    (r"^\d\d\-\d\d\-\d\d\s+\d{1,2}\:\d\d\:\d\d$", r"%y-%m-%d %H:%M:%S"),
    (r"^\d\d\d\d\-\d\d\-\d\d\s+\d{1,2}\:\d\d$", r"%Y-%m-%d %H:%M"),
    (r"^\d{1,2}\/\d{1,2}\/\d\d\d\d\s\d{1,2}\:\d\d", r"%m/%d/%Y %H:%M"),
    (r"^\d\d\d\d\-\d\d\-\d\d\s+\d{1,2}:\d\d:\d\d (AM|PM)", r"%Y-%m-%d %I:%M:%S %p"),
]


def _get_time_format(time):
    for regex, datetime_format in DATETIME_REGEX_FORMATS:
        if re.fullmatch(regex, time):
            return datetime_format
    logger.warning("Unknown datetime format: %s", time)
    return None


def _parse_onset_csv_header(header_lines):
    full_header = "\n".join(header_lines)
    header = {
        "history": "",
        "timezone": re.search(r"GMT\s*([\-\+\d\:]*)", full_header),
        "plot_title": re.search(r"Plot Title\: (\w*),+", full_header),
        "logger_sn": ",".join(set(re.findall(r"LGR S\/N\: (\d*)", full_header))),
        "sensor_sn": ",".join(set(re.findall(r"SEN S\/N\: (\d*)", full_header))),
        "instrument_sn": ",".join(
            set(
                re.findall(r"(?:SEN S\/N|LGR S\/N|Serial Number):\s*(\d+)", full_header)
            )
        ),
        "lbl": ",".join(set(re.findall(r"lbl: (\d*)", full_header))),
    }

    header = {
        key: value[1] if isinstance(value, re.Match) else value
        for key, value in header.items()
    }
    # Handle Columns
    original_columns = list(reader([header_lines[-1]], delimiter=",", quotechar='"'))[0]
    variables = {}
    for col in original_columns:
        # Ignore plot title from column names
        if header["plot_title"]:
            col = col.replace(header["plot_title"], "")

        column_with_units = re.sub(
            r"\s*\(*(LGR S\/N|SEN S\/N|LBL): .*",
            "",
            col,
        )
        column = re.split(r"\,|\(|\)", column_with_units)[0].strip()
        variables[column] = {
            "original_name": col,
            "units": re.split(r"\,|\(", column_with_units.replace(")", "").strip())[
                -1
            ].strip()
            if re.search(r"\,|\(", column_with_units)
            else None,
        }

    header["time_variables"] = [var for var in variables if "Date Time" in var]

    if header["timezone"] is None:
        logger.warning("No Timezone available within this file. UTC will be assumed.")
        header["timezone"] = "UTC"

    return header, variables


def _standardized_variable_mapping(variables):
    """Standardize onset variable names"""
    return {
        var: VARIABLE_NAME_MAPPING[var]
        if var in VARIABLE_NAME_MAPPING
        else var.lower().replace(" ", "_")
        for var in variables
    }


def csv(
    path: str,
    convert_units_to_si: bool = True,
    standardize_variable_names: bool = True,
    encoding: str = "UTF-8",
    errors: str = "strict",
    timezone: str = None,
    ambiguous_timestamps: str = "raise",
) -> xarray.Dataset:
    """Parses the Onset CSV format generate by HOBOware into a xarray object

    Args:
        path: The path to the CSV file
        convert_units_to_si: Whether to standardize data units to SI units
        standardize_variable_names: Rename the variable names a standardize name
            convention
        encoding: File encoding. Defaults to "utf-8"
        errors: Error handling. Defaults to "strict"
        timezone: Timezone to localize the time variable, overwrites the timezone in header
        ambiguous_timestamps: How to handle ambiguous time stamps. Defaults to "raise"
    Returns:
        xarray.Dataset
    """
    raw_header = []
    line = ""
    with open(
        path,
        encoding=encoding,
        errors=errors,
    ) as f:
        while "Date Time" not in line and len(raw_header) < 10:
            line = f.readline()
            raw_header.append(line)
        first_row = f.readline()
    if "Date Time" not in raw_header[-1]:
        raise ValueError("Date Time column not found in header")

    # Parse onset header
    header, variables = _parse_onset_csv_header(raw_header)
    date_column_index = list(variables.keys()).index("Date Time")
    date_format = _get_time_format(first_row.split(",")[date_column_index])

    # Inputs to pd.read_csv
    consider_columns = {
        var: id
        for id, var in enumerate(variables.keys())
        if var.lower().replace(" ", "_") not in IGNORED_VARIABLES
    }
    df = pd.read_csv(
        path,
        na_values=[" "],
        skiprows=list(range(len(raw_header))),
        parse_dates=["Date Time"],
        date_format=date_format,
        sep=",",
        header=None,
        memory_map=True,
        names=consider_columns.keys(),
        usecols=consider_columns.values(),
        encoding_errors=errors,
        encoding=encoding,
    )

    # Add timezone to time variables
    if df["Date Time"].dtype == "object":
        logger.warning(
            "Date Time column is not in a consistent format. Trying to convert"
        )
        df["Date Time"] = df["Date Time"].apply(
            lambda x: pd.to_datetime(x, format=_get_time_format(x))
        )
    df["Date Time"] = df["Date Time"].dt.tz_localize(
        timezone or header["timezone"], ambiguous=ambiguous_timestamps
    )
    check_daylight_saving(df["Date Time"], ambiguous_timestamps)

    # Convert to dataset
    ds = df.to_xarray()
    ds.attrs = {**GLOBAL_ATTRIBUTES, **header}
    for var in ds:
        ds[var].attrs = variables[var]

    if standardize_variable_names:
        ds = ds.rename_vars(_standardized_variable_mapping(ds))
        # Detect instrument type based on variables available
        ds.attrs["instrument_type"] = _detect_instrument_type(ds)

    # # Review units and convert SI system
    if convert_units_to_si:
        if standardize_variable_names:
            if "temperature" in ds and ("C" not in ds["temperature"].attrs["units"]):
                logger.warning("Temperature in Farenheit will be converted to celsius")
                ds["temperature"] = _farenheit_to_celsius(ds["temperature"])
                ds["temperature"].attrs["units"] = "degC"
                ds.attrs["history"] += " ".join(
                    [
                        f"{datetime.now()}",
                        f"Convert temperature ({ ds['temperature'].attrs['units']}) to"
                        "degree Celsius [(degF-32)/1.8000]",
                    ]
                )
            if (
                "conductivity" in ds
                and "uS/cm" not in ds["conductivity"].attrs["units"]
            ):
                logger.warning(
                    "Unknown conductivity units (%s)", ds["conductivity"].attrs["units"]
                )
        else:
            logger.warning(
                "Unit conversion is not supported if standardize_variable_names=False"
            )

    ds = standardize_dataset(ds)
    return ds


def _detect_instrument_type(ds):
    """Detect instrument type based on variables available in the dataset."""
    # Try to match instrument type based on variables available (this information is
    # unfortnately not available withint the CSV)
    vars_of_interest = {
        var
        for var in ds
        if var not in IGNORED_VARIABLES and not var.startswith("unnamed")
    }

    if vars_of_interest == {"temperature", "light_intensity"}:
        instrument_type = "Pendant"
    elif vars_of_interest == {"specific_conductance", "temperature", "low_range"}:
        instrument_type = "CT"
    elif vars_of_interest == {"temperature", "specific_conductance"}:
        instrument_type = "CT"
    elif vars_of_interest == {"temperature"}:
        instrument_type = "Tidbit"
    elif vars_of_interest == {"temperature", "sensor_depth"}:
        instrument_type = "PT"
    elif vars_of_interest == {"temperature", "pressure", "sensor_depth"}:
        instrument_type = "PT"
    elif vars_of_interest == {
        "temperature",
        "barometric_pressure",
        "pressure",
        "sensor_depth",
    }:
        instrument_type = "WL"
    elif vars_of_interest == {
        "temperature",
        "barometric_pressure",
        "pressure",
        "water_level",
    }:
        instrument_type = "WL"
    elif vars_of_interest == {"temperature", "pressure"}:
        instrument_type = "airPT"
    elif vars_of_interest == {"barometric_pressure"}:
        instrument_type = "airP"
    elif vars_of_interest == {"turbidity"}:
        instrument_type = "turbidity"
    else:
        instrument_type = "unknown"
        logger.warning(
            "Unknown Hobo instrument type with variables: %s", vars_of_interest
        )
    return instrument_type


def _farenheit_to_celsius(farenheit):
    """Convert temperature in Farenheit to Celcius
    Args:
        farenheit (float): Temperature in Farenheint

    Returns:
        float: Temperature in celsisus
    """
    return (farenheit - 32.0) / 1.8000


def xlsx(
    path: str, timezone: str = None, ambiguous_timestamps: str = "infer"
) -> xarray.Dataset:
    """Parses the Onset XLSX format generate by HOBOware into a xarray object

    Args:
        path: The path to the XLSX file
        timezone: Timezone to localize the time variable, overwrites the timezone in header
        ambiguous_timestamps: How to handle ambiguous time stamps. Defaults to "infer"
    Returns:
        xarray.Dataset
    """

    def _format_detail_key(key):
        """Format detail key to be more readable"""
        key = re.sub(r"\(.*\)", "", key)
        return (
            key.replace(" Info", "")
            .replace(" ", "_")
            .replace("-", "_")
            .lower()
            .replace("deployment_deployment", "deployment")
            .replace("device_device", "device")
            .replace("app_app", "app")
        )

    def _get_column_and_unit(column):
        """Split column name and unit in parenthesis"""
        column = column.split(" (")
        if len(column) == 1:
            return column[0], None
        return column[0], column[1].replace(")", "")

    # Read the different sheets from the xlsx file
    data = pd.read_excel(path, sheet_name="Data", engine="openpyxl")
    events = pd.read_excel(path, sheet_name="Events", engine="openpyxl")
    details = (
        pd.read_excel(
            path,
            sheet_name="Details",
            engine="openpyxl",
            names=["group", "subgroup", "parameter", "value"],
        )
        .ffill(axis=0)
        .dropna(subset=["parameter", "value"])
    )
    details_attrs = {
        _format_detail_key(f"{row['subgroup']}_{row['parameter']}"): row["value"]
        for id, row in details.iterrows()
        if row["group"] == "Devices"
    }

    variable_attributes = {}

    for var in data.columns:
        column, unit = _get_column_and_unit(var)
        column = _format_detail_key(column)
        if column == "#":
            column = "record_number"
        elif column == "date_time":
            column = "time"
        variable_attributes[column] = {
            "long_name": column,
            "units": unit,
            "original_name": var,
        }
    data.columns = variable_attributes.keys()

    if "time" not in data.columns:
        raise ValueError("Date Time column not found in header")
    file_timezone = variable_attributes["time"].pop("units", None)
    if file_timezone:
        file_timezone = TIMEZONE_MAPPING.get(file_timezone, file_timezone)

    # Convert to dataset
    data["time"] = (
        pd.to_datetime(data["time"], errors="coerce")
        .dt.tz_localize(timezone or file_timezone, ambiguous=ambiguous_timestamps)
        .dt.tz_convert("UTC")
    )
    check_daylight_saving(data["time"])

    ds = data.to_xarray()
    for var in variable_attributes:
        ds[var].attrs = variable_attributes[var]
    ds.attrs = {**GLOBAL_ATTRIBUTES, "events": events.to_json(), **details_attrs}
    ds["instrument_type"] = _detect_instrument_type(ds)
    ds = standardize_dataset(ds)
    return ds
